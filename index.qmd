---
title: "Inside Out: Externalizing Assumptions in Data Analysis as Validation Checks"
abstract: "In data analysis, unexpected results often prompt researchers to revisit their procedures to identify potential issues. While some researchers may struggle to identify the root causes, experienced researchers can often quickly diagnose problems by checking a few key assumptions. These checked assumptions, or expectations, are typically informal, difficult to trace, and rarely discussed in publications. In this paper, we introduce the term *analysis validation checks* to formalize and externalize these informal assumptions. We then introduce a procedure to identify a subset of checks that best predict the occurrence of unexpected outcomes, based on simulations of the original data. The checks are evaluated in terms of accuracy, determined by binary classification metrics, and independence, which measures the shared information among checks. We demonstrate this approach with a toy example using step count data and a generalized linear model example examining the effect of particulate matter air pollution on daily mortality. "
author:
  - name: H. Sherry Zhang
    email: huize.zhang@austin.utexas.edu
    affiliations: 
        - id: 1
          name: University of Texas at Austin
          department: Department of Statistics and Data Sciences
          city: Texas
          country: United States
          postal-code: 78705
    attributes:
        corresponding: true
  - name: Roger D. Peng
    email: roger.peng@austin.utexas.edu
    affiliations: 
        - id: 1
          name: University of Texas at Austin
          department: Department of Statistics and Data Sciences
          city: Texas
          country: United States
          postal-code: 78705
cite-method: natbib
bibliography: references.bib
format:
 tandf-pdf:
   keep-tex: true
   fontsize: 12pt
   linestretch: 1.15
   notebook-links: false
   keywords: diagnostic, logic regression, data analysis assumptions
preamble: >
  \usepackage{amsfonts,amsmath,amssymb,amsthm}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
#remotes::install_github("huizezhang-sherry/adtoolbox")
library(tidyverse)
library(patchwork)
library(MASS)
library(ggh4x)
library(adtoolbox)
library(broom)
library(knitr)
library(rpart)
library(infotheo)
library(patchwork)
library(readr)
library(kableExtra)
library(scales)
library(qqplotr)
pm10 <- read_csv(here::here("data", "pm10.csv")) |> 
  filter(!is.na(pm10)) |>    
  mutate(season = as.factor(season))  
```

# Introduction

In data analysis, analysts often rely on their prior knowledge or domain expertise to form expectations and assess whether results align with them. When results deviate from these expectations, experienced analysts check assumptions made about the data and during the analysis, but often do so without discussing the underlying reasoning in the publications. While publication of analysis code and data is now a common a requirement for the sake of reproducibility [@peng2011reproducible], the published code alone is often insufficient for understanding the thought process behind an analysis. Code corresponding to published analyses often reflect the final decisions made about analysis and do not reveal the decisions or assumptions made about the data during the analysis. Thus, a reader looking at published code can often be left with many questions about why certain choices were made.

We might gain insight into analysts' thought processes by speaking with them directly or watching them work via screencast videos they produce, such as TidyTuesday screencast videos or think-aloud type studies [e.g. @gu2024data]. However, direct observation of analysis is not scalable and may not always be feasible; creating educational screencast videos requires significant effort from the analysts. Ideally, there could be a way to explicitly communicate these thought processes in the data analysis to others. Even better, if these expectations were machine-readable, we could analyze them and learn from the analysis itself. For example, we could answer questions about whether the checks also apply to other researchers analyzing new data in the same context, whether they reflect common practices in the field, or whether they are specific to the data or analysis at hand. Externalizing these thought processes can improve the trustworthiness of the analyses [@yu2024veridical]. 

The trustworthiness of an analysis depends not only on the robustness of the methods used but also on a deep understanding of the data. This corresponds to the third step identified in @broderick_toward_2023, where trust can break down if the assumptions underlying an algorithm are not satisfied in practice. In the past, data were often collected directly by analysts through carefully designed experiments, however, contemporary analyses often rely on data curated by government or research institutions, where some data characteristics may be unknown to the analysts. When unexpected outcomes arise, they often later traced back to implicit assumptions made by the analysts - assumptions that were unknowingly violated by the idiosyncratic in the data. By externalizing these assumptions through analysis validation checks, we aim to make it easier to identify and diagnose such unexpected results.  

In this paper, we conceptualize these implicit expectations as *analysis validation checks*, which allows us to examine the assumptions made about the data during an analysis. We then introduce a procedure to identify the subset of checks that best predict the occurrence of unexpected outcomes. The procedure, based on simulations of the original data, compares the accuracy and independence of different combinations of analysis validation checks. Accuracy is determined using binary classification metrics (precision and recall) from a logic regression model [@ruczinski_logic_2003], while independence measures the shared information among checks. The proposed workflow offers a numerical guarantee that the analysis will produce the expected results, assuming the assumptions about the data generating mechanism hold.

The rest of the paper is organized as follows: @sec-lit-review reviews the concepts of diagnosing unexpected outcomes and general data quality checks. @sec-plan introduces the concept of analysis validation checks, illustrated with a toy example based on step count data. @sec-method describes the procedure that selects the best subset of checks for understanding the occurrence of unexpected outcomes. @sec-pm10-mortality applies this procedure to a larger example that estimates the effect of particulate matter air pollution on daily mortality. @sec-discussion summarises the paper and discusses a few key considerations.

# Related Work {#sec-lit-review}

## Diagnosing unexpected outcomes in data analysis

The concept of framing data analysis as a sense-making process was originally presented by @grolemund_cognitive_2014 based on seminal work by @wild1999statistical. Key to any sense-making process is a model for the world (i.e. expectations for what we might observe) and observed data with which we can compare our expectations. If there is a significant deviation between what we observe and our expectations, then a data analysis must determine what is causing that deviation. A naive approach would be to update our model for the world to match the data, under the assumption that the initial expectation was incorrect. However, experienced analysts know that the reality can be more nuanced than that, with errors occurring in data collection or data processing that can have an impact on final results. 

The skill of diagnosing unexpected data analysis results is not one that has received significant attention in the statistics literature. While the concept of diagnosis is often embedded in model checking or data visualization techniques, systematic approaches to identifying the root cause of a general unexpected analysis result are typically not presented [@peng2022perspective]. Furthermore, if interesting information is discovered through model checking or data visualization, there is no formal way to document such discoveries for the next analyst to consider. @peng_diagnosing_2021 proposed a series of exercises for training students in data analysis to diagnose different kinds of analysis problems such as coding errors or outliers. They provide a systematic approach involving working backwards from the analysis result to identify potential causes. There are parallels here to the concept of debugging and testing in software engineering [@donoghue2021teaching]. For example, @li2019towards found that experienced engineers were generally able to identify problems in code faster than novices, and that the ability to debug code required knowledge that cut across different domains. 

If it is true that the speed with which data analysts can identify problems with an analysis is related to their experience working with a given type of data, then there is perhaps room to improve the analytic process by externalizing the aspects that an analyst learns through experience. That way, inexperienced analysts could examine the thought process of an experienced analyst and learn to identify factors that can cause unexpected results to occur.

## Data analysis checks 

The concept of data quality has been studied in the literature, with earlier work focusing on defining frameworks -- such as dimensions, attributes, and measures -- to improve data quality in database and information systems [@wang1996beyond; @batini2009methodologies; @6204995; @woodall2014classification; @cai2015challenges; @8642813]. More recently, data validation has been incorporated into frameworks like Google TensorFlow [@polyzotis2019data] to ensure the quality of data for training machine learning models, as well as for supporting business decision-making [@schelter2018automating]. With the increasing availability of open data in scientific research, the users of the data are often not the original data collector and may not be aware of all the details or nuances of the data. This encourages researchers to conduct data quality checks before beginning the analytic process, helping to avoid unexpected discoveries later on. In R, packages like `skimr` [@skimr] and `dataMaid` [@dataMaid] provide basic data screening and data quality reports, while packages such as  `assertr` [@assertr], `validate` [@validate], and `pointblank` [@pointblank] focus on providing infrastructures that allow users to define customized data quality checks.


The literature on data quality typically focuses on the intrinsic or inherent quality of the data themselves, rather than the data's relationship to any specific data analysis. So for example, if a column in a data table is expecting numerical data, but we observe a character value in one of the entries, then that occurrence would trigger some sort of data quality check. This type of quality check can be triggered without any knowledge of what the data will ultimately be used for. However, for a given analysis, we may require specific aspects of the data to be true because they affect the result being computed. Conversely, certain types of poor quality data may have little impact on the ultimate result of an analysis (e.g. data that are missing completely at random). Defining data quality in terms of what may affect a specific analysis outcome or result has the potential to open new avenues for defining data checks and for building algorithms for optimizing the collection of checks defined for a specific analysis.

# Analysis validation checks {#sec-plan}

<!-- Expectations represent our understanding of certain aspects of the analysis and the data, independent of the results of the analysis itself. When observed outcomes deviate from these expectations, analysts often revisit the analysis process to identify potential issues, refine methods, or revise assumptions. Experienced analysts can typically identify issues quickly and correct them on the spot, but they often do so without discussing the underlying reasoning, limiting transparency and making it harder for less experienced researchers to learn and master these skills.  -->

Analysis validation checks are assumptions made by the analysts during the analysis, framed as explicit checks that return a binary TRUE or FALSE value based on the data. Inspired by the concept of data validation checks [@validate], which are designed to ensure that datasets meet expected quality before the analysis begins, analysis validation checks reverse the approach: they validate the assumptions about the data necessary for the analysis to produce the *expected results*, as defined by the analyst. The focus on expected results allows analysis validation checks to encompass a wide range of checks, such as data quality, variable distributions and outliers, bivariate and multivariate relationships, and contextual information relevant to the analysis. 

Our proposed analysis validation checks provide insights into an analyst's thought process and offer the following benefits: 

  1. Serve as clear checkpoints to support the replication or application of methods to (new) data by programmatically communicating the requirements or assumptions made of the data;
  3. Align assumptions among researchers from different domain backgrounds who may have different expectations about the data;
  4. Improve analysis transparency, reproducibility, and trustworthiness by externalizing a key part of the analysis process; and
  5. Quantify the effectiveness of analysis checks for predicting the expected outcome (see @sec-method);
  
In addition to the above benefits, the development and publication of analysis checks have the potential to help students, inexperienced analysts, and junior researchers develop the skills needed to diagnose unexpected analysis results for a given type of data because the assumptions made about the data are made transparent. The analysis checks can serve as a basis for new analysts to have conversations about the data they are analyzing and to develop a better understanding of the potential data generation process.


## A Toy Example {#sec-toy}

Consider a 30-day step count experiment in a public health setting. Subjects are instructed to walk at least 8,000 steps each day, with an expected average of 9,000 steps, tracked by a step counter app. With data of this nature, we may expect there to be occasional "low" days due to factors such as unfavorable weather conditions limiting outdoor activities.  We may also expect "high" days recorded after an outdoor activities or intense workout. Given the requirements of the study, we form our expectation as follows:

  > Expectation: The average step count of a given subject is between $[8,500, 9,500]$

To diagnose potential reasons why this expectation might fail, we can establish a few analysis validation checks in anticipation of seeing the data. [TODO: Add guidelines for designing the checks] For example, we can check the quantile of the step count, if more than a third of the days fall below 8,000, or more than a third exceed 10,000 steps, this could indicate an excess of low-count or high-count days. Similarly, we may may expect the standard deviation of the step count not to be overly large. These considerations yield the following three analysis validation checks that *fail* when:

-   Check 1: the 60% quantile of the observed step counts is greater than 10,000
-   Check 2: the 40% quantile of the observed step counts is less than 8,000, and
-   Check 3: the standard deviation of the observed step counts exceeds 2,500.

The cutoff values for these checks would presumably be chosen based on prior experience with these kinds of data, but could also be optimized using the method presented in the next section.

To simulate this data, three normal distributions are used for the daily step counts: $\mathcal{N}(6,000, 200)$ for low days, $\mathcal{N}(12,000, 200)$ for high days, and $\mathcal{N}(9,000, 300)$ for typical days. The number of low and high days can be simulated from a Poisson distribution with $\lambda = 8$. @fig-step-count displays average step count across 300 simulated 30-day periods.

```{r}
#| fig-height: 3
#| label: fig-step-count
#| fig-cap: Beeswarm plot of average step counts across 300 simulated 30-day periods. Each point represents the average step count from one simulation. Similar to a boxplot or violin plot, the beeswarm plot also displays the distribution of each individual data point. The orange points indicate instances where the average step count fails outside the [8,500, 9,500] interval, representing an unexpected outcome in this scenario. 
set.seed(1234)
step_count <- tibble(id = 1:300) |> 
  rowwise() |> 
  mutate(
    small_n = rpois(1, 8),
    large_n = rpois(1, 8),
    norm_sd = rlogis(1, 800, scale = 100),
    norm_n = 30 - small_n - large_n,
    step = list(round(c(rnorm(small_n, 6000, 200), 
                        rnorm(large_n, 12000, 200), 
                        rnorm(norm_n, 9000, 300)
    ), 0)),
    mean = mean(step, na.rm = TRUE),
    unexpect = ifelse(between(mean, 8500, 9500), 0, 1),
    `q(step, 0.6) > 10000` = ifelse(quantile(step, 0.6, na.rm = TRUE) > 10000, 1, 0),
    `q(step, 0.4) < 8000` = ifelse(quantile(step, 0.4, na.rm = TRUE) < 8000, 1, 0),
    `sd(step) > 2500` = ifelse(sd(step) > 2500, 1, 0),
  ) |>
  ungroup()

step_count |> 
  ggplot(aes(x = mean, y = 1, color = as.factor(unexpect))) + 
  ggbeeswarm::geom_quasirandom() + 
  scale_y_continuous(breaks = seq(0, 13, 1)) +
  scale_color_brewer(palette = "Dark2") + 
  xlab("Average number of steps") + 
  ylab("") + 
  theme_bw() + 
  theme(legend.position = "none", panel.grid.minor = element_blank(), axis.text.y = element_blank())
```

# Method {#sec-method}

While not all analysis validation checks are equally important, some may be more useful at diagnosing unexpected outcomes than others. This section introduces a procedure to identify a subset of analysis validation checks that best predict the occurrence of unexpected outcomes. 

The approach generates simulated datasets from the observed data to compute the outcome and the analysis validation checks (unexpected as 1 and expected as 0). This then allows us to apply a logic regression tree to link the outcome and the check results to construct a Boolean expression to best predict the unexpected outcomes. @fig-metric-calc provides an overview of the process. 

The specific strategy to simulate the data depends on the problem and the nature of the data at hand. Simulated data can be generated using prior domain knowledge, summary statistics derived from the observed data, or statistical techniques such as training-testing splits or bootstrap sampling. While each simulated data may not be independent of the original data - as is the case with bootstrap samples - it should adequately capture the range of possible idiosyncrasies present in the data. Failure to do so can reduce the ability of the resulting Boolean expression to predict unexpected outcomes. 

```{r}
#| label: fig-metric-calc
#| out.width: 100%
#| fig-cap: "Overview of the procedure to quantify the effectiveness of analysis validation checks. The procedure involves simulating replicates of the data, applying the anlysis, and running the analysis validation checks on each replicate to determine whether the outcome meets the expectation (the unexpected column) and whether each check passes (check 1, check 2, ...). The simulation results are then passed to two branches: the accuracy branch calculates the precision and recall of the checks from a logic regression prediction, while the redundancy branch to calculate the mutual information and independence of the checks. These three metrics are combined to produce a single metric that quantifies the effectiveness of the checks in diagnosing unexpected outcomes in the analysis."
knitr::include_graphics(here::here("figures", "metric-calc.png"))
```

## Accuracy

From the simulated data, the accuracy branch in @fig-metric-calc refers to the set of checks' ability to detect unexpected outcomes accurately. To relate multiple checks to the outcome, a logic regression model [@ruczinski_logic_2003] is used. Originally developed for SNP micro-array data, logic regression predict classification or regression outcomes by constructing Boolean combinations, $\mathcal{L}(X_1, X_2, \cdots, X_k)$, where $X_i$ are binary predictors. The logical operations allowed to construct $\mathcal{L(\cdot)}$ includes AND ($\land$), OR ($\lor$), and NOT ($\neg$) and an example of the Boolean combination of $\mathcal{L(\cdot)}$ is $X_1 \text{ AND } (X_2 \text{ OR } X_3)$. In our binary-binary classification problem, the logic regression model can be written as follows:

$$E(Y) = \mathcal{L}(X_1, X_2, \cdots, X_k),$$

where the outcome, $Y$, indicates whether the outcome is *unexpected* (1) or *expected* (0), and the analysis validation checks, $X_1, X_2, \cdots, X_k$, are labeled as 1 if they fail and 0 if they pass. 

This objective function is optimized using simulated annealing algorithm to minimize the misclassification error and the following 6 moves are permitted during the optimization to grow or prune the tree (as detailed in Figure 2 of @ruczinski_logic_2003): 1) replacing a leaf node, 2) replacing an operator, 3) growing a branch, 4) pruning a branch, 5) splitting a leaf node, and 6) deleting a leaf node. Predictions from the logic regression model is then used, along with the observed outcome, to calculate the precision and recall of the optimized Boolean combination of checks. Compared to other tree-based methods for binary-binary prediction, the Boolean combinations from the logic regression model produce a tree structure that can be directly interpreted as the possible combination of checks leading to an unexpected outcome, without the need to invert the tree as required in classic tree-based recursive partitioning methods. 

<!-- -   precision: the proportion of correctly identified unexpected results (true positives) out of all the predicted unexpected results (true positives + false positives) -->
<!-- -   recall: the proportion of correctly identified unexpected results (true positives) out of all the actual unexpected results (true positives + false negatives) -->

## Independence

While checks may score high on predictive accuracy, they may be correlated and less useful to provide actions to act on the analysis. This could happen if a set of checks are all tangentially related to the cause of the unexpected results, but none addresses the root cause. To quantify the redundancy among the checks, we use total correlation, $C$, a multivariate generalization of mutual information that captures the amount of shared information across two variables. We define the normalized total correlation, $C'$, as 

$$C'(X_1, X_2, \cdots, X_k) = \frac{C(X_1, X_2, \cdots, X_k)}{\sum_{i= 1}^k H(X_i)} = \frac{\sum_{i= 1}^k H(X_i) - H(X_1, X_2, \cdots, X_k)}{\sum_{i= 1}^k H(X_i)}, $$
where $H(X_i)$ is the entropy of the $i$-th check and $H(X_1, X_2, \cdots, X_k)$ is the joint entropy of checks $\{X_1, X_2, \cdots, X_k\}$. The amount of independent information, $\eta$, can be defined as:

$$\eta(X_1, X_2, \cdots, X_k) = 1 - C'(X_1, X_2, \cdots, X_k) = \frac{H(X_1, X_2, \cdots, X_k)}{\sum_{i= 1}^k H(X_i)}.$$

To scale this quantity to the range $[0,1]$ for consistency with other metrics (i.e., precision and recall), we define the independence metric as:

$$\text{independence} = \frac{\eta - 1/k}{1 - 1/k}= \frac{H(X_1, X_2, \cdots, X_k)/\sum_{i= 1}^K H(X_i) - 1/k}{1 - 1/k}.$$

An independence metric value of 1 indicates that all checks are independent: $H(X_1, X_2, \cdots, X_k)=\sum_{i= 1}^k H(X_i)$ and each check provides unique information to diagnose the unexpected outcome (the trivial case is when there is only one check). A value of 0 indicates the checks are all identical: $H(X_1, X_2, \cdots, X_k)- \sum_{i= 1}^k H(X_i) = H(X_i)/ k H(X_i) = 1/k$. 


The three metrics (precision, recall, and independence) can be combined into a single metric using the arithmetic mean, harmonic mean, or quadratic mean. The differences among these means are minimal when the three metrics are similar. However, as the differences among the metrics increases, the harmonic mean tends to produce the smallest overall score, as it penalizes low values, while the quadratic mean tends to produce the largest score by rewarding higher values more. For simple interpretation of the score, the arithmetic mean is preferred, while in applications where the difference between precision, recall, and independence need to be penalized or rewarded more, the harmonic and quadratic mean should be considered.

## Toy Example Revisited

Returning to the step count example introduced in @sec-toy, the logic regression model is fitted to the three validation checks described previously to predict the outcome, whether the average number of step falls within the [8,500, 9,500] interval. @fig-logic-reg shows the best-fitting logic regression model as

  > (quantile(step, 0.6) > 10,000 OR quantile(step, 0.4) < 8,000) AND (NOT sd(step) > 2,500)

We would predict an unexpected outcome in the analysis if the standard deviation of the step count is not too large (2,500) and either the 60% quantile of the step count exceeds 10,000 or the 40% quantile of the step count falls below 8,000.

```{r}
#| label: fig-logic-reg
#| fig.width: 8
#| fig.heigh: 2
#| fig-cap: Logic regression model fitted to the three checks. The model suggests the rule (quantile(step, 0.6) > 10,000 OR quantile(step, 0.4) < 8,000) AND (NOT sd(step) > 2,500). The NOT operator applied to `sd(step) > 2,500` is colored with a black background to distinguish it from other checks.
fit <- step_count |> fit_logic_reg(unexpect, `q(step, 0.6) > 10000`:`sd(step) > 2500`, seed = 1, nleaves = 3)
plot_logicreg(fit)
```

```{r}
fit_regtree <- rpart(unexpect ~ ., data = step_count |> dplyr::select(unexpect:`sd(step) > 2500`), method = "class", 
                     control = rpart.control(cp = 0.01)) 
pred_vec <- predict(fit_regtree) |> as_tibble() |> mutate(.fitted = ifelse(`0` > `1`, 0, 1)) |> dplyr::select(.fitted)
regtree_df <- step_count |> dplyr::select(unexpect) |> bind_cols(pred_vec) |> 
  calc_miscla_rate(unexpect, .fitted) 

sum_entropy <- lapply(as.list(step_count[,c(9:11)]),  function(x){infotheo::entropy(x, method = "emp")}) |> unlist() |> sum()
multi_info <- infotheo::multiinformation(step_count[,c(9:11)])
joint_entropy <- sum_entropy - multi_info
ratio <- joint_entropy / sum_entropy

regtree_df <- regtree_df |> 
  mutate(independence =  (ratio - 1/3) / (1 - 1/3)) |> 
  calc_metrics(metrics = c("harmonic", "arithmetic"))

sum_entropy <- lapply(as.list(step_count[,c(9,11)]),  function(x){infotheo::entropy(x, method = "emp")}) |> unlist() |> sum()
multi_info <- infotheo::multiinformation(step_count[,c(9,11)])
joint_entropy <- sum_entropy - multi_info
ratio <-  joint_entropy / sum_entropy

another <- tibble(.fitted = as.vector(step_count[,9] & !step_count[,11]) |> as.numeric(), 
                  unexpect = step_count$unexpect) |> 
  calc_miscla_rate(unexpect, .fitted) |>
  mutate(independence = (ratio - 1/2) / (1 - 1/2)) |> 
  calc_metrics(metrics = c("harmonic", "arithmetic")) |> 
  mutate(id = 6)
```

@tbl-logic-reg presents the calculated precision, recall, and independence for the three individual checks and the check rule found by the logic regression. The harmonic and arithmetic means are included to combine the three measures. The results show that the three checks produced by the logic regression can accurately predict 86.1% cases of all *predicted unexpected results* in the simulation data. Furthermore, 43.1% of all *actual unexpected results* were in fact observed to be unexpected.  

```{r}
#| label: tbl-logic-reg
#| tbl-cap: "Precision, recall, and independence calculated for each individual check and the logic regression check rule. The harmonic and arithmetic means of the three metrics are included to evaluate the quality of the checks in  diagnosing unexpected step counts (more than five days with fewer than 8,000 steps)." 
list(tibble(.fitted = step_count$`q(step, 0.6) > 10000`, unexpect = step_count$unexpect),
     tibble(.fitted = step_count$`q(step, 0.4) < 8000`, unexpect = step_count$unexpect),
     tibble(.fitted = step_count$`sd(step) > 2500`, unexpect = step_count$unexpect)
) |> 
  map_dfr(~.x |> calc_miscla_rate(unexpect, .fitted) |> 
            mutate(independence = 1) |> 
            calc_metrics(metrics = c("harmonic", "arithmetic"))) |> 
  mutate(checks = c("Check 1: q(step, 0.6) > 10000", "Check 2: q(step, 0.4) < 8000", "Check 3: sd(step) > 2500")) |>
  bind_rows(augment(fit) |> 
              calc_miscla_rate(.fitted, unexpect) |> 
              calc_independence() |> 
              calc_metrics(metrics = c("harmonic", "arithmetic")) |> 
              mutate(checks = "Logic regression: (check 1 OR check 2) AND (not check 3)")) |> 
  bind_rows(another  |>
              mutate(checks = "Comparison: (check 1) AND (not check 3)")) |>
  bind_rows(regtree_df |>
              mutate(checks = "Regression tree")) |>
  dplyr::select(checks, precision:arithmetic) |> 
  dplyr::rename(`indep.` = independence) |> 
  kable(digits = 3, escape = FALSE, format = "latex", booktab = TRUE, 
        col.names = c("Checks", "Precision", "Recall", "Independence", "Harmonic", "Arithmetic")) |> 
  kableExtra::kable_styling(latex_options="scale_down") |> 
  column_spec(1, width = '19em')
```

For comparison, we include a simplified rule, (quantile(step, 0.6) > 10,000) AND (NOT sd(step) > 2,500). Although the checks on the 60th quantile and standard deviation are less correlated -- reflected in a higher independence score -- this pair is less accurate at predicting the unexpected outcome, resulting in lower harmonic and arithmetic mean scores. The regression tree produces a similar prediction to the logic regression, however, we argue that the logic regression tree shown in @fig-logic-reg is more interpretable for our purposes because it provides a direct representation of which combinations of analysis checks lead to unexpected outcomes. The logic regression tree is also directly comparable to other diagnostic techniques, which we discuss further in @sec-discussion.

# Application {#sec-pm10-mortality}

In the study of the health effects of outdoor air pollution, one area of interest is the association between short-term, day-to-day changes in particulate matter air pollution and daily mortality counts. Substantial work has been done to study this question and to date, there appears to be strong evidence of an association between particulate matter less than 10 $\mu$g/m$^3$ in aerodynamic diameter (PM10) and daily mortality from all non-accidental causes [@samet2000fine]. In the following example, we use the problem of studying PM10 and mortality along with data from the National Morbidity, Mortality, and Air Pollution Study (NMMAPS) to demonstrate how our analysis validation checks described in @sec-method can be applied. In addition to providing a more substantial problem for our methods, this example also demonstrates how the procedure presented in @sec-method can be used to select cutoff values in the analysis checks to diagnose an unexpected PM10 coefficient from the generalized linear model. The dataset that we use to develop our simulations and analysis checks is from New York City, and contains daily PM10, all-cause (non-accidental) mortality, and average temperature values from 1992--2000. We then will apply the analysis checks to similar data from 39 other cities from the NMMAPS study.

The typical approach to studying the association between PM10 and mortality is to apply a generalized linear model (Poisson family with a log link) to relate daily mortality counts to daily measures of PM10. Based on previous work and the range of effect sizes published in the literature, an analyst might expect the coefficient for PM10 in this GLM to lie between $[0, 0.005]$, after adjusting for daily temperature [@samet2000fine; @welty2005acute]. Note that the relatively simple modeling approach being used here is primarily for demonstration; typically far more sophisticated semi-parametric approaches are used in the literature [@peng2006model]. 

Multiple factors can affect the estimated PM10 coefficient, such as the strength of the correlation between mortality and PM10, or between mortality and temperature. Outliers in the variables can also leverage the coefficient. While these are possible factors that could affect the analysis result, it is not clear what the cutoff values for these checks should be to determine a failure. Here we consider a list of checks in @tbl-checks with varied cutoff values: 

| The check fails (encoded as 1) if ... | 
|--------------| 
| Mortality-PM10 correlation less than $-0.05$ |
| Mortality-PM10 correlation less than $-0.03$ |
| Mortality-PM10 correlation greater than $0.03$ |
| Mortality-PM10 correlation greater than $0.05$ |
| Mortality-temperature correlation greater than $-0.3$ | 
| Mortality-temperature correlation greater than $-0.35$ | 
| Mortality-temperature correlation greater than $-0.4$ | 
| Mortality-temperature correlation greater than $-0.45$ | 
| Outlier(s) are presented in the variable PM10 | 
| Outlier(s) are presented in the variable mortality | 

: A list of checks considered for the generalized linear model of mortality on PM10 and temperature. The checks are based on the sample size, correlation between mortality and PM10, correlation between mortality and temperature, and univariate outlier detection. Multiple cutoff values are specified for each check to determine a failure. {#tbl-checks} 


## Data Simulation

To generate replicates of the dataset, we first generate the correlation matrix of the three variables (PM10, mortality, and temperature) in a grid and then use a Gaussian copula to generate a multivariate normal distribution based on the specified correlation matrix and sample size. The multivariate normal distribution is transformed using the normal CDF before the inverse CDF of the assumed distributions of the three variables is applied. To determine the appropriate distribution of each variable, various distributions are fitted and compared. This includes poisson and negative binomial for mortality; gamma, log-normal, exponential, weibull, and normal for PM10 and temperature; and beta for PM10 after rescaling the data to $[0,1]$. 

To ensure a reasonable likeness to data that might be used in such an analysis, we use characteristics of the observed New York City dataset to refine our simulations. AIC is used to determine the best distribution fit for each variable with the QQ-plot presented in @fig-dist-fit to evaluate the fit. AIC suggests a negative binomial distribution for mortality, a beta distribution for PM10 (multiple by 100 to recover the original scale), and a Weibull distribution for temperature. To include the potential effect of outliers, we add a single outlier to the data for both the mortality and PM10 variables.

```{r}
#| label: fig-dist-fit
#| fig-cap: "QQ-plot of the goodness-of-fit of selected distributions used to generate simulated data for mortality, PM10, and temperature. Left: Mortality as a negative binomial distribution (size = 74, mean = 183), Middle: PM10 is modeled using a rescaled beta distribution with shape parameters 4.21 and 11.67, multiplied by 10 to match the observed range. Right: Temperature is modeled using a Weibull distribution (shape = 3.8, scale = 61)"
#| fig.height: 4
#| fig.width: 10
aa2 <- fitdistrplus::fitdist(pm10$mortality, "nbinom")
p1 <- tibble(fitted = rnbinom(558, size = aa2$estimate[1], mu = aa2$estimate[2]),
       observed = pm10$mortality) %>%
  ggplot(aes(sample = fitted)) +
  stat_qq_band(fill = "grey80") +
  stat_qq_line(color = "grey30") +
  stat_qq_point(size = 0.3) +
  theme_bw() + 
  theme(aspect.ratio = 1) + 
  ggtitle("Mortality")


aa6 <- fitdistrplus::fitdist(pm10$pm10[pm10$pm10 < 90]/100, "beta")
p2 <- tibble(fitted = rbeta(557, shape1 = aa6$estimate[1], shape2 = aa6$estimate[2]) * 100,
       observed = pm10$pm10[pm10$pm10 < 90]) %>%
  ggplot(aes(sample = fitted)) +
  stat_qq_band(fill = "grey80") +
  stat_qq_line(color = "grey30") +
  stat_qq_point(size = 0.3) +
  theme_bw() + 
  theme(aspect.ratio = 1) +
  ggtitle("PM10")
  

aa4 <- fitdistrplus::fitdist(pm10$temp, "weibull")
p3 <- tibble(fitted= rweibull(558, aa4$estimate[1], aa4$estimate[2]),
       observed= pm10$temp) %>%
  ggplot(aes(sample = fitted)) +
  stat_qq_band(fill = "grey80") +
  stat_qq_line(color = "grey30") +
  stat_qq_point(size = 0.3) +
  theme_bw() + 
  theme(aspect.ratio = 1) +
  ggtitle("Temperature")

p1 | p2 | p3
```

```{r}
# cor(pm10$pm10, pm10$mortality)
# cor(pm10$temp, pm10$mortality)
# cor(pm10$temp, pm10$pm10)
corr_grid <- expand.grid(#seq(0.01, 0.2, by = 0.02),
                         seq(-0.1, 0.1, by = 0.02),
                         seq(0.1, 0.5, by = 0.05),
                         seq(0.25, 0.45, by = 0.05))

gen_corr_mtx <- function(r1, r2, r3) {
  # correlation between mortality and pm10 are negative - r1
  # correlation between mortality and temp are negative - r2
  cor_matrix <- matrix(c(1, r1, -r2,
                         r1, 1, r3,
                         -r2, r3, 1), nrow = 3, byrow = TRUE)
  if (all(eigen(cor_matrix)$values > 0)) return(cor_matrix)
}

corr_mtx <- lapply(1:nrow(corr_grid), function(i) {
  gen_corr_mtx(corr_grid[i, 1], corr_grid[i, 2], corr_grid[i, 3]) })
corr_mtx <- corr_mtx[map_lgl(corr_mtx, ~!is.null(.x))]
sample_size <- c(500, 3000)
outlier <- c(TRUE, FALSE)

generate_data <- function(n, mtx, seed = 123, outlier = FALSE) {
  mu <- c(0, 0, 0)
  set.seed(seed)
  data <- mvrnorm(n, mu, mtx, empirical = TRUE)
  U <- pnorm(data, mean = 0, sd = 1)

  if (!outlier) {
    tibble(mortality = qnbinom(U[,1], size = 74, mu = 183),
           pm10 = qbeta(U[,2], shape1 = 4.21, shape2 = 11.67) * 100,
           temp = qweibull(U[,3], shape = 3.8, scale = 61))
  } else{
    pm10_vec <- qbeta(U[,2], shape1 = 4.21, shape2 = 11.67) * 100
    tibble(
      mortality = c(qnbinom(U[,1], size = 74, mu = 183)[-1],
                    rnorm(n = 1, mean = 300, sd = 10)),
      pm10 = c(pm10_vec[-1], rnorm(n = 1, mean = 100, sd = 10)),
      temp = qweibull(U[,3], shape = 3.8, scale = 61)
    )
  }

}

res <- tibble(corr_mtx = corr_mtx) |>
  mutate(id = row_number()) |>
  crossing(sample_size, outlier) |>
  rowwise() |>
  mutate(data = list(generate_data(n = sample_size, mtx = corr_mtx, outlier = outlier)),
         fit = list(summary(glm(mortality ~ pm10 + temp, family = "poisson", data))$coefficients))

pm10_df <- res |>
  mutate(
    p_value = fit[2,4],
    coef = fit[2,1],
    mor_pm10_correlation = corr_mtx[1, 2],
    mor_temp_correlation = corr_mtx[1, 3],
    temp_pm10_yz_correlation = corr_mtx[2, 3],
    unexpect = ifelse(between(coef, 0, 0.005), 0, 1),
    `cor(m, PM10) < -0.05` = ifelse(mor_pm10_correlation < -0.05, 1, 0),
    `cor(m, PM10) < -0.03` = ifelse(mor_pm10_correlation < -0.03, 1, 0),
    `cor(m, PM10) > 0.03` = ifelse(mor_pm10_correlation > 0.03, 1, 0),
    `cor(m, PM10) > 0.05` = ifelse(mor_pm10_correlation > 0.05, 1, 0),
    `cor(m, tmp) > -0.3` = ifelse(mor_temp_correlation > -0.3, 1, 0),
    `cor(m, tmp) > -0.35` = ifelse(mor_temp_correlation > -0.35, 1, 0),
    `cor(m, tmp) > -0.4` = ifelse(mor_temp_correlation > -0.4, 1, 0),
    `cor(m, tmp) > -0.45` = ifelse(mor_temp_correlation > -0.45, 1, 0)
  ) |>
  ungroup()

pm10_df <- pm10_df |>
  rowwise() |>
  mutate(PM10_outlier = ifelse(any(scale(data$pm10) > 4), 1, 0),
         mortality_outlier = ifelse(any(scale(data$mortality) > 4), 1, 0)
  ) |>
  ungroup()

unexpect_df <- tibble(
  unexpect = c(0, 1),
  n = pm10_df$unexpect |> table() |> as.numeric() |> rev(),
  wgt = n/sum(n))
wgt_vec <- pm10_df |> dplyr::select(unexpect) |> left_join(unexpect_df) |> pull(wgt)
```

A logic regression is fitted using all variations of the checks in @tbl-checks to predict whether the PM10 coefficient is unexpected. Given the imbalance of outcome (expected vs. unexpected), inverse weights proportional to the number of observations in each outcome are applied during the logic regression fit. @fig-linear-reg-tree shows the optimal logic regression tree from the fitted model. Precision, recall, and independence score, along with their harmonic and arithmetic mean are presented in @tbl-linear-reg. 


::: {#fig-linear-reg-tree}

```{r}
#| fig.width: 12
#| fig.height: 6
fit <- pm10_df |> fit_logic_reg(unexpect, `cor(m, PM10) < -0.05`:mortality_outlier,
                                seed = 7, nleaves = 4, wgt = wgt_vec)
plot_logicreg(fit)
```

Logic regression model fitted to the fourteen checks and the outcome expectation (unexpected) as the response variable. The model suggests the relationship:  (mortality-PM10 correlation < $-0.03$) AND (mortality-temperature correlation > $-0.3$ OR (there exist mortality outlier AND there exist PM10 outlier)) to predict the unexpected PM10 coefficient.
:::



```{r}
#| label: tbl-linear-reg
#| tbl-cap: "Precision, recall, and independence metrics derived from the logic regression model and each individual check, along with harmonic and arithmetic means."
df <- list(
  tibble(.fitted = pm10_df$`cor(m, PM10) < -0.03`, 
         unexpect = pm10_df$unexpect),
  tibble(.fitted = pm10_df$`cor(m, tmp) > -0.3`, 
         unexpect = pm10_df$unexpect),
  tibble(.fitted = pm10_df$mortality_outlier, 
         unexpect = pm10_df$unexpect),
  tibble(.fitted = pm10_df$PM10_outlier, 
         unexpect = pm10_df$unexpect)) |>
  map_dfr(~.x |> calc_miscla_rate(.fitted, unexpect) |> 
            mutate(independence = 1) |> 
            calc_metrics(metrics = c("harmonic", "arithmetic")), .id = "id") |> 
  mutate(id = as.numeric(id))

reg <- fit |> augment() |>
  calc_miscla_rate(unexpect, .fitted) |>
  calc_independence() |> 
  calc_metrics(metrics = c("harmonic", "arithmetic")) |> 
  mutate(id = 5)

df <- df |> bind_rows(reg)

tbl <- tibble(check = c("Check 1: cor(m, PM10) < -- 0.03", "Check 2: cor(m, tmp) > -- 0.35",
                        "Check 3: mortality outlier", "Check 4: PM10 outlier", 
                        "Logic regression: (check 1) AND ((check 2) OR (check 3 AND check 4))")) |> 
  bind_cols(df |> dplyr::select(-id)) |> rename(`indep.` = independence) 

tbl |> 
  kable(digits = 3, escape = FALSE, format = "latex", booktab = TRUE,
        col.names = c("Checks", "Precision", "Recall", "Independence", "Harmonic", "Arithmetic")) |> 
  kableExtra::kable_styling(latex_options="scale_down") |> 
  column_spec(1, width = '18em')

```

As indicated in @fig-linear-reg-tree, the logic regression model picks up the following cutoff values for each type of check:

-   mortality-PM10 correlation less than $-0.03$
-   mortality-temperature correlation greater than $-0.3$
-   PM10 data contains outliers detected by the univariate outlier detection
-   mortality data contains outliers detected by the univariate outlier detection

The fitted logic regression model is 

  > (cor(mortality, PM10) < -- 0.03) AND (cor(mortality, tmp) > -- 0.3 OR (mortality outlier AND PM10 outlier))

This generates a precision of `r round(as.numeric(df[5, "precision"]), 3)` and a recall of `r round(as.numeric(df[5, "recall"]), 3)` for predicting the unexpected PM10 coefficient. As shown in @fig-linear-reg-tree, there is no single analysis check in the tree that predicts an unexpected outcome. rather at least three checks in the tree must be TRUE in order for the model to predict an unexpected outcome. Given the high independence of the checks (@tbl-linear-reg), this suggests that unexpected results are only likely after multiple anomalies are observed in the data.

## Additional Cities 

In order to test our development of analysis validation checks based on the New York City data, we incorporate data from 39 additional cities from the original NMMAPS study and apply our checks to those data. These cities (along with New York City) represent approximately the largest 40 metropolitan areas in the United States by population. The complete list of cities used are presented in @tbl-all-cities. For each city, we had daily data on all-cause mortality, PM10, and average temperature for the years 1992--2000.

```{r message=FALSE}
#| label: tbl-all-cities
#| tbl-cap: Results from full analysis of 40 cities from the NMMAPS air pollution and mortality study. In the table, a "T" indicates that a check failed and an "F" indicates that a check passed.

pm10 <- read_csv("data/pm10.csv") |> filter(!is.na(pm10)) |> filter(!is.na(temp))
files <- list.files(path = "data", pattern = "csv", full.names = TRUE)[-1]
pred_res <- files |> map_dfr(function(raw){
  data <- read_csv(raw) |> filter(!is.na(pm10)) |> filter(!is.na(temp))
  res_df <- glm(mortality ~ pm10 + temp, family = "poisson", data = data)$coefficients |>
    as_tibble_row()

  res_df |>
      mutate(
        cor_m_tmp = cor(data$mortality, data$temp),
        cor_m_pm10 = cor(data$mortality, data$pm10),
        cor_tmp_pm10 = cor(data$temp, data$pm10),
        unexpect = !between(pm10, 0, 0.005),
        mort_outlier = any(scale(data$mortality) > 4),
        pm10_outlier = any(scale(data$pm10) > 4),
        lgl_cor_m_tmp = cor(data$mortality, data$temp) > -0.3,
        lgl_cor_m_pm10 = cor(data$mortality, data$pm10) < -0.03,
        .pred = ((mort_outlier &  pm10_outlier) | (lgl_cor_m_tmp)) & (lgl_cor_m_pm10),
        ss = nrow(data)
        )
    })
city_name_map <- read_csv(here::here("data/city_name_map.csv"))


df <- tibble(city = str_remove(files, "data/pm10-") |> str_remove(".csv")) |> bind_cols(pred_res) |>
  left_join(city_name_map) |> 
  filter(city != "data/pm10") |> 
  dplyr::select(cityname, pm10, unexpect, .pred, 
                cor_m_tmp, lgl_cor_m_tmp,                
                cor_m_pm10, lgl_cor_m_pm10, 
                mort_outlier, pm10_outlier)

df |>
  mutate(pm10 = format(round(pm10, 4), nsmall = 4),
         cor_m_tmp = format(round(cor_m_tmp, 2), nsmall = 2),
         cor_m_pm10 = format(round(cor_m_pm10, 2), nsmall = 2),
         across(c(pm10, cor_m_pm10, cor_m_tmp), ~str_replace(.x, "-", "--"))) |> 
  mutate(across(c(unexpect, .pred, lgl_cor_m_pm10, lgl_cor_m_tmp, mort_outlier, pm10_outlier), 
                ~ifelse(.x, "T", "F"))) |>
  mutate(cor_m_tmp = paste0(cor_m_tmp, " (", lgl_cor_m_tmp, ")"),
         cor_m_pm10 = paste0(cor_m_pm10, " (", lgl_cor_m_pm10, ")")) |> 
  dplyr::select(-lgl_cor_m_tmp, -lgl_cor_m_pm10) |> 
  kable(format = "latex", booktabs = TRUE, escape = F, 
        align = "lrccrrcc",
        col.names = linebreak(c("City", 
                                "PM10\nEstimated\nCoefficient",
                                "Outcome\nObserved\nUnexpected", 
                                "Logic Regression\nPredicted\nUnexpected", 
                                "Correlation:\nMortality-\nTemperature",
                                "Correlation:\nMortality-\nPM10",
                                "Mortality\nOutlier", 
                                "PM10\nOutlier"))) |>
  kableExtra::kable_styling(latex_options="scale_down") 
```

We applied each of the analysis validation checks and the fitted logic regression model separately to each of the cities' data to make predictions of whether the outcome will be unexpected or not. We also applied the generalized linear model to the data from each of the cities to estimate the association between PM10 and mortality in each city, adjusted for temperature. The estimates of the association and the results of each of the analysis validation checks for each city are shown in @tbl-all-cities. 

```{r}
#| label: tbl-accuracy
#| tbl-cap: Summary of the observed and the predicted unexpected PM10 coefficient results from the 40 NMMAPS cities using the logic regression model.


df |> 
  dplyr::select(unexpect, .pred) |> 
  table() |> 
  as_tibble() |> 
  rename(Observed = unexpect) |> 
  pivot_wider(names_from = .pred, values_from = n) |> 
  kable(format = "latex", 
        booktabs = TRUE) |> 
   add_header_above(c("", "Predicted"=2), 
                    escape = FALSE) 
```

```{r,include=FALSE}
cities_accurate <- df |> 
  filter(unexpect, .pred) |> 
  pull(cityname)
cities_missed <- df |> 
  filter(unexpect, !.pred) |> 
  pull(cityname)
```

It is clear from @tbl-all-cities that some of the estimated PM10 coefficients are outside the expected range of [0, 0.005]. Eight of the 40 cities had negative estimated coefficients and were therefore considered unexpected by our original criterion. (It is perhaps worth noting that none of the unexpected outcomes was in the positive direction.) @tbl-accuracy summarizes the prediction accuracy of the logic regression model for the 40 cities. The model correctly predicts the PM10 coefficient status of `r accurate <- df |> filter(unexpect == .pred) |> nrow(); accurate` cities, producing an accuracy rate of `r label_percent()(accurate/nrow(df))`. Among the `r df |> filter(unexpect) |> nrow()` cities with an unexpected outcome, the model correctly identifies `r df |> filter(unexpect, .pred) |> nrow()` (`r cities_accurate`), giving a recall of `r recall <- as.numeric(df |> calc_miscla_rate(.pred, unexpect) |> pull(recall)); label_percent()(recall)` (only `r cities_missed` was not properly classified by the logic regression model). These cities were flagged due to failures in both the mortality-PM10 correlation and mortality-temperature correlation analysis checks. Out of the `r df |> filter(.pred) |> nrow()` cities with positive predictions from the model, `r df |> filter(unexpect != .pred, !unexpect) |> nrow()` cities (`r df |> filter(unexpect != .pred, !unexpect) |> pull(cityname)`) were false positives, resulting in a precision of only `r pres <- as.numeric(df |> calc_miscla_rate(.pred, unexpect) |> pull(precision)); label_percent()(pres)`. This precision value is substantially lower than what was estimated by the simulation procedure (see @tbl-linear-reg) and suggests that the simulation does not adequately capture some features of the data generation process.



# Discussion {#sec-discussion}

In this paper we have developed an approach to using analysis validation checks to externalize the assumptions about the data and analysis tools made during the data analysis process. These checks can serve as a useful summary of the analyst's thought process and can describe how characteristics of the data may lead to unexpected outcomes. Using logic regression, we can develop a graphical summary of the analysis validation checks as well as use the logic regression fitting process to choose the optimal set of checks. The logic regression model can also be used to develop summaries of the precision and recall of the collection of analysis validation checks in predicting the likelihood of an unexpected outcome. We demonstrated our method on an example relating daily mortality to outdoor air pollution data. The results from that example could be used to inform future analyses of air pollution and health data, perhaps in other cities or locations.

In @sec-pm10-mortality we used the analysis checks in a diagnostic manner to examine the eight cities whose PM10 coefficients were unexpected. There, we found that seven of the cities had a mortality-PM10 correlation that was more negative than expected while also having a mortality-temperature correlation that was somewhat larger (less negative) than expected. What this result implies for the broader analysis depends on a number of factors, but the interplay between mortality, PM10, and temperature may warrant further investigation [see e.g. @welty2005acute]. Note that simply because the PM10 coefficient estimates were unexpected by our criterion, we do not mean to imply that they are ``wrong" in any sense. Indeed, negative estimated coefficients have been found in other studies of this nature [@bell2004ozone]. Rather, in this type of analysis, it may be that the interval for expected results needs to be revised. Further, it may be necessary to incorporate other outputs from the analysis, such as measures of uncertainty.

An interesting connection can be drawn between our logic regression trees and a tool used in systems engineering known as a fault tree. A fault tree is used for conducting a structured risk assessment and has a long history in aviation, aerospace, and nuclear power applications [@vesely1981fault]. A fault tree is a graphical tool that describes the possible combinations of causes and effects that lead to an anomaly. At the top of the tree is a description of an anomaly. The subsequent branches of the tree below the top event indicate possible causes of the event immediately above it in the tree. The tree can then be built recursively until we reach a root cause that cannot be further investigated. Each level of the tree is connected together using logic gates such as AND and OR gates. The leaf nodes of the tree indicate the root causes that may lead to an anomaly. While the logic regression tress are not identical to fault trees, they share many properties, such as the tree-based structure and the indicator of root causes at the leaf nodes. Perhaps more critically, both serve as graphical summaries of the assumptions made in a problem and the specific violations of those assumptions that could lead to an unexpected result. While fault trees are often used to discover the root cause of an anomaly after it occurs, an important use case for fault trees is to develop a comprehensive understanding of a system *before* an anomaly occurs [@michael2002fault].

Visualization methods are also valuable tools for assessing data assumptions and can potentially be formalized as analysis validation checks. For instance, plotting a variable's distribution using a histogram, density plot, or bee swarm plot can reveal outliers or deviations from normality. These visualizations could be re-framed as analysis checks that fail when the data does not conform to the visual expectation. However, translating visualization results into binary checks remains an open challenge, requiring either manual verification or the development of automated methods to interpret visualization outputs. An existing example of visual test is the R package `vdiffr` [@vdiffr] for graphic software unit testing. The package saves a template plot and compares it to the current plot to determine whether the unit tests passes or fails.

Systematically generating realistic simulated data is a key component of our approach and is deserving of further consideration. In the PM10 example in @sec-pm10-mortality, the inverse transform method was used to preserve the correlation structure among mortality, PM10, and temperature. However, the simulation process can become complex when additional restrictions are imposed or when a greater range of scenarios is desired. In such cases, techniques like the acceptance-reject method or permutation may be used to generate the data. The results in @sec-pm10-mortality on the broader NMMAPS dataset suggest that our simulation procedure may have been inadequate in reflecting the range of possible configurations that the data could take. In particular, our split-data approach, using New York City to guide the simulation and then applying the analysis checks to other cities, may not have been ideal. It may be worth exploring some recent work in data thinning [@neufeld2024data], data fission [@leiner2023data], or differential privacy methods [@dong_gaussian_2022] to create training datasets that are more representative of future data. The Gaussian copula used in the example doesn't account for the tail dependence between the variables and other copula, e.g. t-copula or extreme value distribution copula, could also be explored to better capture the tail dependence.

Analysis validation checks are closely related to the concept of unit testing in software engineering. While unit tests isolate and test specific lines of the code, analysis validation checks focus on the assumptions underlying the analysis rather than the explicit code itself. Moreover, while software testing is deterministic, with clear rules for determining failure, analysis validation checks are probabilistic. As a result, an analysis may fail several assumption checks yet produce an expected outcome, or pass all checks but yield an unexpected result.

Communicating the process of data analysis is a key element to providing transparency, improving reproducibility, and building trust with a range of audiences. The analysis validation checks described here provide a general way to encode the assumptions that an data analyst makes about the data and statistical tools applied to the data. These assumptions can be studied or challenged, depending on the specific analyst's perspective, and can serve as a roadmap for diagnosing unexpected results.

# Acknowledgement

The article is created using Quarto [@Allaire_Quarto_2022] in R [@R].
The source code for reproducing the work reported in this paper can be
found at: <https://github.com/huizezhang-sherry/paper-avc>. 

# Supplementary material {.unnumbered}

The supplementary materials include a full script of the examples in the paper (`index.R`) and its output (`index.html`), the data used for in the examples in Section 5 (`data/`), the package source (`adtoolbox_0.1.0.tar.gz`), and a README.md file containing the install instructions for running the scripts.

# References
