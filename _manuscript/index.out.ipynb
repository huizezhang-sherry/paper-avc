{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My wonderful paper\n",
    "\n",
    "H. Sherry Zhang (University of Texas at Austin)  \n",
    "Roger D. Peng (University of Texas at Austin)\n",
    "\n",
    "This is the abstract"
   ],
   "id": "0048262f-f274-411a-b1d5-a0ad2ccbe782"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n",
      "✔ dplyr     1.1.4     ✔ readr     2.1.5\n",
      "✔ forcats   1.0.0     ✔ stringr   1.5.1\n",
      "✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n",
      "✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n",
      "✔ purrr     1.0.2     \n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Attaching package: 'MASS'\n",
      "\n",
      "The following object is masked from 'package:patchwork':\n",
      "\n",
      "    area\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    select"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "Attaching package: 'ggh4x'\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    guide_axis_logticks"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Rows: 3288 Columns: 6\n",
      "── Column specification ────────────────────────────────────────────────────────\n",
      "Delimiter: \",\"\n",
      "chr  (1): season\n",
      "dbl  (4): mortality, pm10, temp, dewpt\n",
      "date (1): date\n",
      "\n",
      "ℹ Use `spec()` to retrieve the full column specification for this data.\n",
      "ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
     ]
    }
   ],
   "source": [
    "knitr::opts_chunk$set(echo = FALSE)\n",
    "library(tidyverse)"
   ],
   "id": "2fb7d5ef-aba7-46f6-9570-f0101be86e5d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\\[background\\]\n",
    "\n",
    "In this paper, a concept called analysis plan is proposed to describe the logical structure of a data analysis. An analysis plan is a set of analysis steps plus their expected outcomes. It is a formal representation of the analysis process and can be used to guide the analysis process, to communicate and compare the analysis process to others, and to evaluate the analysis process. The concept of analysis plan is illustrated with examples. The implications of the concept for data analysis practice is discussed.\n",
    "\n",
    "The analysis plan described in this paper should be differentiated from the pre-specifies analysis plan document often used in biostatistics to specifies the hypothesis, data collection mechanism, statistical procedures etc of randomized experiments.\n",
    "\n",
    "The rest of the paper is organized as follows: <a href=\"#sec-plan\" class=\"quarto-xref\">Section 2</a> describes the concept of analysis plan in detail. <a href=\"#sec-examples\" class=\"quarto-xref\">Section 3</a> provides examples of analysis plan \\[more details\\]. (need another section here or before examples?) <a href=\"#sec-conclusion\" class=\"quarto-xref\">Section 4</a> concludes the paper.\n",
    "\n",
    "# Analysis plan\n",
    "\n",
    "-   describe/ define what analysis plan is, expectation (outcome and plan)\n",
    "\n",
    "-   why it is useful to be explicit about expectation?\n",
    "\n",
    "    -   expectations can be used to formulate unit tests, which helps to divide the “result universe”\n",
    "\n",
    "# Examples\n",
    "\n",
    "Three examples are presented to illustrate how the concept of analysis plan can be applied to data analysis. \\[toy example\\]. <a href=\"#sec-linear-reg\" class=\"quarto-xref\">Section 3.2</a> illustrates how constructing the result universe in a linear regression model of PM10 on mortality can help understand the impact of sample size, model specification, and variable correlation structure on data analysis. \\[example three\\]\n",
    "\n",
    "## A toy example\n",
    "\n",
    "Let’s think about a 5-day step count. You make a resolution to walk on average 5000 steps a day (your expectation) and using an app to record your step count. After 5 days, the app tells you’ve walked on average 8000 steps.\n",
    "\n",
    "It is easy to come up with reasons why an 8000 average step is resulted based on common sense:\n",
    "\n",
    "1.  you may run a 10k on day 1, resulting a high step count on the day (outlier on the right).\n",
    "2.  you left your phone at home on day 3, resulting a zero or minimal step count on the day (outlier on the left).\n",
    "3.  you may realise the step count may increase since you were in a hiking trip in the last five days (average shift).\n",
    "\n",
    "Based on these reasons, you may devise a set of unit tests to check the step count data, i.e. check the maximum and minimum step count, check the difference between each day.\n",
    "\n",
    "-   If the daily count looks like c(4000, 5000, 5500, 5500, 20000), the maximum check will flag the data for investigate the maximum. The difference between days test will also flag the data\n",
    "-   If the daily count looks like c(20000, 20000, 20000, 20000, 20000), the maximum check will flag the data for investigate the maximum.\n",
    "\n",
    "Some part of the space is impossible: c(0, 4000, 5000, 5500, 5500) is flagged by the minimal tests but won’t cause an average of 8000 average step.\n",
    "\n",
    "The statistical procedure of averaging 5 numbers “around 5000” to get a mean of 5000 is *consistent* meaning if all the numbers are around 5000, we are guaranteed to get a mean around 5000. We could devise 5 unit tests to check each number. Since you’re more familiar with your daily life, you may realise the step count may increase since you were in a hiking trip in the last two days. This may prompt you to check the step count.\n",
    "\n",
    "In a data analysis, it is not practical to check every entry of the data, a similar strategy of devising tests to check for\n",
    "\n",
    "-   The combination of unit tests are not unique\n",
    "-   The unit tests provide guidance for diagnosing the results, but are not red flags: c(2000, 2000, 5000, 8000, 8000) will likely to fail the max diff test but receive a within expectation mean.\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "Consider a linear regression model to study the effect of PM10 on mortality (provide context of using PM10 to study mortality). Analysts may expect a significant (p-value $\\le$ 0.05) PM10 coefficient in the linear model from the literature. This is the *outcome expectation*. There are multiple factors that can affect the outcome expectation of linear regression, which here is called *plan expectation*, for example, 1) sample size, 2) model specification, and 3) correlation structure between variables. Adequate sample size is required to achieve the desired power to detect the significance of PM10 on mortality. Temperature is often an important confounder to consider in such study (add reference). From some domain knowledge, an analyst may expect that the significance of PM10 coefficient can be attained by adding temperature to the model. Analysts may also expect certain correlation structure between PM10, temperature, and mortality, and the distribution of each variable.\n",
    "\n",
    "To build the result universe, datasets can be simulated to either meet and fail these plan expectations, allowing the analysts to observe the significance of PM10 coefficient. Here, sample sizes of 50, 100, 500, and 1000 are considered. Two model specifications are included: 1) linear model with PM10 as the only covariate ($\\text{mortality} \\sim \\text{PM10}$), 2) linear model with PM10 and temperature as covariates ($\\text{mortality} \\sim \\text{PM10} + \\text{temp}$). A grid-based approach is used to simulate correlation structure. Reasonable ranges of correlation between the three variables are $\\text{cor}(\\text{mortality}, \\text{PM10}) \\in [-0.01, 0]$, $\\text{cor}(\\text{mortality}, \\text{temperature}) \\in [-0.6, -0.2]$, and $\\text{cor}(\\text{PM10}, \\text{temperature}) \\in [0.2, 0.6]$.\n",
    "\n",
    "-   add a paragraph to describe the simulation process\n",
    "\n",
    "-   add a fourth panel to describe the comparison of a right/ wrong expectation, i.e. correlation on PM10 and mortality\n",
    "\n",
    "<a href=\"#fig-result-universe\" class=\"quarto-xref\">Figure 1</a> shows that result universe of the linear regression model and how a change of decision in one of the plan expectations above affect the outcome expectation. Panel a) is colored by the outcome expectation – whether a significant p-value is found in the PM10 coefficient. Panel b) shows the effect of adding temperature to the model and the results show that the significance of PM10 coefficient can be achieved by adding temperature to the model for a sample size of 500. Panel c) shows that increasing sample size from 50 to 100 enhances the significance of p-value for PM10 and the significance remains with further increases in sample size. \\[note: weave the “actual data” into the example linear regression model\\]\n",
    "\n",
    "(these two paragraphs may go to a new section *discussion*) A result universe constructed in this example can be presented to analysts to answer the what-if questions raised in the data analysis. What if the sample size is increased? What if temperature is added to the model? What would the results expect to be changed when the correlation structure is different? For analysts, expectations can be used as unit tests to divide the result universe, which allows them to understand the impact of each factor on the outcome expectation.\n",
    "\n",
    "The result universe also provides a holistic view of how the results obtained by the analysts are situated in all possible results. This can be seen as a direct towards trustworthy data analysis, where the audience of the analysis to exercise their own cognitive model \\[@grolemund_cognitive_2014\\] to evaluate the results reported."
   ],
   "id": "61dc4f86-e960-4c08-bac8-d9804deb8db1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation matrices \n",
    "corr_grid <- expand.grid(seq(-0.01, -0.001, by = 0.001), \n",
    "                         seq(-0.6, -0.2, 0.05), seq(0.2, 0.6, 0.05))\n",
    "\n",
    "# Function to compute correlation matrix for each combination \n",
    "gen_corr_mtx <- function(r1, r2, r3) {   \n",
    "  cor_matrix <- matrix(c(1, r1, r2,                          \n",
    "                         r1, 1, r3,                          \n",
    "                         r2, r3, 1), nrow = 3, byrow = TRUE)          \n",
    "  # Store the matrix in the list   \n",
    "  if (all(eigen(cor_matrix)$values > 0)) return(cor_matrix) \n",
    "  }  \n",
    "\n",
    "# Plan for parallel processing\n",
    "corr_mtx <- lapply(1:nrow(corr_grid), function(i) {   \n",
    "  gen_corr_mtx(corr_grid[i, 1], corr_grid[i, 2], corr_grid[i, 3]) \n",
    "  }) \n",
    "corr_mtx <- corr_mtx[map_lgl(corr_mtx, ~!is.null(.x))]  \n",
    "sample_size <- c(50, 100, 500, 1000) \n",
    "model <- c(\"mortality ~ pm10 + temp\", \"mortality ~ pm10\") |> map(as.formula) \n",
    "\n",
    "generate_data <- function(n, mtx, seed = 123) {   \n",
    "  mu <- c(0, 0, 0)   \n",
    "  data <- mvrnorm(n, mu, mtx, empirical = TRUE)   \n",
    "  U <- pnorm(data, mean = 0, sd = 1)   \n",
    "  set.seed(seed)      \n",
    "  tibble(mortality = qpois(U[,1], 182), # assume distribution\n",
    "         pm10 = qgamma(U[,2], shape = 6, rate = 0.2),           \n",
    "         temp = qnorm(U[,3], mean = 55, sd = 16))   \n",
    "  }  \n",
    "\n",
    "# test for dist fit\n",
    "# fitdist(pm10$pm10, \"gamma\") -> pm10_fit\n",
    "# plot(pm10_fit) # gamma is better than normal for the fit of pm10\n",
    "#fitdist(pm10$temp, \"norm\") -> temp_fit\n",
    "# plot(temp_fit)\n",
    "\n",
    "res <- tibble(corr_mtx = corr_mtx) |> \n",
    "  mutate(id = row_number()) |> \n",
    "  crossing(sample_size, model) |> \n",
    "  rowwise() |>   \n",
    "  mutate(data = list(generate_data(n = sample_size, mtx = corr_mtx)),          \n",
    "         fit = list(summary(lm(model, data))$coefficients))  "
   ],
   "id": "7b6413f8-a774-4731-bf78-035a22d9c413"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "dt <- res |>   \n",
    "  mutate(     \n",
    "    p_value = fit[2,4],     \n",
    "    coef = fit[2,1],     \n",
    "    xy_correlation = as.numeric(corr_mtx[1, 2]),     \n",
    "    xz_correlation = as.numeric(corr_mtx[1, 3]),     \n",
    "    yz_correlation = as.numeric(corr_mtx[2, 3]),     \n",
    "    fml = deparse(model) |> as.factor(),     \n",
    "    expect = ifelse(p_value < 0.05, 1, 0) |> as.factor(),     \n",
    "    ) |>    \n",
    "  ungroup() \n",
    "\n",
    "code_tbl <- crossing(V1 = c(\"signifpm10\", NA),           \n",
    "                     V2 = c(\"smallsample\", NA), \n",
    "                     V3 = c(\"withtemp\", NA)) |>      \n",
    "  mutate(col1 = paste0(V1,\"_\", V2, \"_\", V3))   \n",
    "\n",
    "lookup_tbl <- tibble(   \n",
    "  value = crossing(x = c(1, 0), y = c(1, 0), z = c(1, 0)) |>      \n",
    "    mutate(col = paste0(x,\"_\", y, \"_\", z)) |> \n",
    "    arrange(-x, -y, -z) |>      \n",
    "    pull(col),   \n",
    "  plan1 = unique(code_tbl$col1) )  \n",
    "\n",
    "dt2 <- dt |>    \n",
    "  mutate(T1 = ifelse(sample_size < 200, 1, 0),          \n",
    "         T2 = ifelse(fml == \"mortality ~ pm10 + temp\", 1, 0),          \n",
    "         pp1 = paste0(expect, \"_\", T1, \"_\", T2),) |>    \n",
    "  left_join(lookup_tbl, by = c(\"pp1\" = \"value\"))      \n",
    "\n",
    "p0 <- dt |> ggplot() +   \n",
    "  geom_point(aes(x = p_value , y = coef), color = \"grey\", size = 0.5) +  \n",
    "  labs(x = \"p-value\", y = \"coefficient\") +   \n",
    "  theme_bw() +    \n",
    "  theme(aspect.ratio = 1, \n",
    "        panel.grid.minor = element_blank()) \n",
    "\n",
    "df0 <-  dt |> filter(sample_size == 500, xy_correlation == -0.005, \n",
    "                     between(xz_correlation, -0.46, -0.43),\n",
    "                     yz_correlation == 0.4) |> arrange(expect)\n",
    "df2 <- df0 |> filter(fml == \"mortality ~ pm10 + temp\") # test for model spec\n",
    "\n",
    "p1 <- p0 + \n",
    "  geom_point(aes(x = p_value , y = coef, color = expect), size = 0.3) +  \n",
    "  scale_color_brewer(palette = \"Dark2\", direction = -1) +\n",
    "  labs(title = \"Coefficient vs. p-value\") + \n",
    "  theme(legend.position = 'none')\n",
    "\n",
    "p2 <- p0 + \n",
    "  geom_point(data = df0, aes(x = p_value, y = coef, color = expect), size = 2) +\n",
    "  geom_path(data = df0, aes(x = p_value, y = coef), color = \"black\",\n",
    "            arrow = arrow(type = \"open\", angle = 30, \n",
    "                          length = unit(0.1, \"inches\"))) +\n",
    "  ggrepel::geom_label_repel(\n",
    "    data = df0, aes(x = p_value, y = coef, label = fml, color = expect)\n",
    "    ) +\n",
    "  scale_color_brewer(palette = \"Dark2\", direction = -1) +\n",
    "  labs(title = \"Effect of adding temperature\") \n",
    "\n",
    "df00 <-  dt |> filter(xy_correlation == -0.005, \n",
    "                     between(xz_correlation, -0.46, -0.43),\n",
    "                     yz_correlation == 0.4) |> arrange(expect)\n",
    "df <- df00 |> filter((fml == \"mortality ~ pm10 + temp\"))\n",
    "\n",
    "\n",
    "p3 <- p0 + \n",
    "  geom_point(data = df, aes(x = p_value, y = coef, color = expect), size = 2) +\n",
    "  geom_path(data = df, aes(x = p_value, y = coef), color = \"black\",\n",
    "            arrow = arrow(type = \"open\", angle = 30, \n",
    "                          length = unit(0.1, \"inches\"))) +\n",
    "  ggrepel::geom_label_repel(\n",
    "    data = df, aes(x = p_value, y = coef, label = sample_size, color = expect)\n",
    "    ) +\n",
    "  scale_color_brewer(palette = \"Dark2\", direction = -1) +\n",
    "  labs(title = \"Effect of sample size\") \n",
    "\n",
    "(p1 | p2) / (p3 | plot_spacer()) + \n",
    "  plot_annotation(tag_levels = \"a\") + \n",
    "  plot_layout(guides = 'collect') &\n",
    "  theme(legend.position = 'bottom') "
   ],
   "id": "cell-fig-result-universe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "# References"
   ],
   "id": "5a5d1d96-827e-4917-84fc-d5ba34bc660e"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
